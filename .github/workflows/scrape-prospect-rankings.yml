name: Scrape Prospect Rankings

on:
  schedule:
    # Daily at 07:00 UTC (1:00 AM CT) â€” offset 1h from draft-order at 06:00
    - cron: '0 7 * * *'
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: scraper-prospect-rankings
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Cargo registry and build artifacts
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            back-end/target
          key: ${{ runner.os }}-cargo-rankings-scraper-${{ hashFiles('back-end/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-rankings-scraper-

      - name: Build scraper
        working-directory: back-end
        run: cargo build --release -p prospect-rankings-scraper

      - name: Scrape Tankathon
        working-directory: back-end
        continue-on-error: true
        run: |
          ./target/release/prospect-rankings-scraper \
            --source tankathon \
            --year 2026 \
            --output data/rankings/tankathon_2026.json

      - name: Scrape WalterFootball
        working-directory: back-end
        continue-on-error: true
        run: |
          ./target/release/prospect-rankings-scraper \
            --source walterfootball \
            --year 2026 \
            --output data/rankings/walterfootball_2026.json

      - name: Merge rankings
        working-directory: back-end
        run: |
          PRIMARY=data/rankings/tankathon_2026.json
          SECONDARY=data/rankings/walterfootball_2026.json
          if [ -f "$PRIMARY" ] && [ -f "$SECONDARY" ]; then
            ./target/release/prospect-rankings-scraper \
              --merge \
              --primary "$PRIMARY" \
              --secondary "$SECONDARY" \
              --output data/rankings/rankings_2026.json
          elif [ -f "$PRIMARY" ]; then
            echo "Only primary source available; copying as merged output."
            cp "$PRIMARY" data/rankings/rankings_2026.json
          elif [ -f "$SECONDARY" ]; then
            echo "Only secondary source available; copying as merged output."
            cp "$SECONDARY" data/rankings/rankings_2026.json
          else
            echo "ERROR: Both scrapers failed; no ranking files produced." >&2
            exit 1
          fi

      - name: Validate JSON output
        working-directory: back-end
        run: |
          for f in data/rankings/tankathon_2026.json data/rankings/walterfootball_2026.json data/rankings/rankings_2026.json; do
            if [ -f "$f" ]; then
              if ! python3 -m json.tool "$f" > /dev/null 2>&1; then
                echo "ERROR: $f is not valid JSON" >&2
                exit 1
              fi
              echo "Validated: $f"
            fi
          done

      - name: Check for changes
        id: changes
        run: |
          if git diff --quiet back-end/data/rankings/; then
            echo "changed=false" >> "$GITHUB_OUTPUT"
          else
            echo "changed=true" >> "$GITHUB_OUTPUT"
          fi

      - name: Commit and push
        if: steps.changes.outputs.changed == 'true'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add back-end/data/rankings/tankathon_2026.json
          git add back-end/data/rankings/walterfootball_2026.json
          git add back-end/data/rankings/rankings_2026.json
          git commit -m "Update prospect rankings data ($(date -u +%Y-%m-%d))"
          git push
